The input of MT-DETR has 4 kinds of sensors: Camera, Lidar, Radar, and Time.

To get the input data, please download STF dataset (Seeing through fog), and follow the instrction of the Github repo (https://github.com/princeton-computational-imaging/SeeingThroughFog).

1. Camera
cam_stereo_left_lut in STF dataset provides the camera data we need, please move them into cam_stereo_left_lut/ here.

2. Lidar
lidar_hdl64_strongest in STF dataset provides lidar pointcloud.
With the visualization tools in the STF dataset, we can obtain 2d lidar points projection with different colors representing distances. Please move them into lidar_projection/

3. Radar
radar_targets in STF dataset provides radar pointcloud.
With the visualization tools in the STF dataset, we can obtain 2d radar points projection with different colors representing distances. Please move them into radar_projection/

4. Time
Run create_time_data.py to create time image for each data, and save them in time_image/.



Synthesize foggy data.
Follow the steps below to generate foggy camera image and foggy lidar image.
1. Foggy camera image
(1) Follow this repo (https://github.com/isl-org/DPT) to generate depth image and save it in depth_image/.
(2) Run 程式.py to generate foggy camera image.

2. Lidar image
Follow this repo (https://github.com/MartinHahner/LiDAR_fog_sim) to generate foggy lidar image.





Briefly introduce the directories under this folder (data folder):

cam_stereo_left_lut: store camera image data
lidar_projection: store lidar image data
radar_projection: store radar image data
time_image: store time image data
splits: record the weather condition for each data
coco_annotation: record the object position and classification for each bounding box
create_time_data.py: used to create time image

depth_image: store depth image data
foggy_camera_image: store foggy camera image data
foggy_lidar_image: store foggy lidar image data
程式.py: used to create foggy camera image

prepare_data.txt: this file

